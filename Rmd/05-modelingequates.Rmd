\newpage

```{r}
# suppress "summarise()` ungrouping output" message
options(dplyr.summarise.inform = FALSE) 
```

# Modelling equates {#ch:modellingequates}

```{=html}
<!-- > Author: Stef van Buuren, Iris Eekhout -->
```

This section deals with the nitty-gritty of the modelling strategy used for the GCDG data introduced in Section \@ref(ch:data). This section

-   provides a high-level description of the GCDG data (\@ref(sec:gcdgdata))
-   discusses various modelling strategies (\@ref(sec:modellingstrategies))
-   shows the impact of equate groups on the model in extreme cases (\@ref(sec:impactequate))
-   demonstrates visualisation of age profiles to select promising equate groups (\@ref(sec:ageprofiles))
-   introduces a helpful visualisation of the quality of the equate group (\@ref(sec:equatequality))
-   highlights infit and outfit for removing misfitting milestones (\@ref(sec:milestoneselection))
-   discusses instrument fit and equate group editing (\@ref(sec:othermodelling))
-   introduces a grading system for equate groups (\@ref(sec:iteminformation))
-   provides pointers to the final model (\@ref(sec:finalmodel))

## GCDG data: design and description {#sec:gcdgdata}

### Data combination

Section \@ref(sec:gcdgoverview) provides an overview of the data collected by Global Child Development Group. The group collected item level measurements obtained on 12 instruments for measuring child development across 16 cohorts.

We coded every item as 0 (FAIL), 1 (PASS) or missing. For some instrument we did some additional recoding to restrict to these two response categories. The Battelle Developmental Inventory scores items as 0 (FAIL), 1, or 2, depending on the level of skill demonstrated or time taken to complete the task. We joined categories 1 and 2 for these items. The ASQ items were originally scored as 0 (not yet), 5 (sometimes) and 10 (succeeds). We recoded both 5 and 10 to 1.

We concatenated the datasets from the GCDG cohorts cohort. The resulting data matrix has 71403 rows (child-visit combinations) and 1572 columns (items) collected from 36345 unique children. We removed 233 items that had fewer than 10 observations in a category. The remaining 1339 items were candidates for analysis. The total number of observed scores was equal to about 2.8 million pass/fail responses. While this is a large number of measurements, about 97 percent of the entries in the matrix are missing.

### Equate group formation

A group of 13 subject-matter experts from the Global Child Development Group cross-walked the available instruments for similar milestones. This group

-   developed an item coding schema;
-   matched similarly appearing items stemming from different instruments;
-   formed an opinion about the quality of each match;
-   noted peculiarities of the matches;
-   reported the results as a series of detailed Excel spreadsheets.

```{r equatesheetpic, echo=FALSE, fig.cap='(ref:equatesheetpic)'}
knitr::include_graphics("fig/equate_excel.png")
```

(ref:equatesheetpic) A snapshot of information generated by subject-matter experts.

The group evaluated around 1500 milestones. After several days, this highly-skilled, intensive labour resulted in a series of spreadsheets. Figure \@ref(fig:equatesheetpic) shows an example. These sheets formed the basis of an initial list of 184 equate groups, each consisting of at least two items.

## Modelling strategies {#sec:modellingstrategies}

The analytic challenge is twofold:

-   to find a subset of items that form a scale;
-   to find a subset of equate groups with items similar enough to bridge instruments.

Note that both subsets are related, i.e., changing one affects the other. Thus, we cannot first identify items and then equate groups, or first identify equate groups followed by the items. Rather we need to find the two subsets in an iterative fashion, primarily by hand. This section describes some of the modelling issues the analyst needs to confront.

In general, we look for a final model that

-   preserves the items that best fit the Rasch model;
-   uses active equate groups with items that behave the same across many cohorts and instruments;
-   displays reasonable age-conditional distributions of the D-scores;
-   has difficulty estimates that are similar to previous estimates.

The modelling strategy is a delicate balancing act to achieve all of the above objectives. Particular actions that we could take to improve a given model are:

-   remove bad items;
-   inactivate bad equate groups;
-   break up bad equate groups;
-   move items from one equate group to another;
-   create new equate groups;
-   remove entire instruments;
-   remove persons;
-   remove studies.

In order to steer our actions, we look at the following diagnostics (in order of importance):

-   quality of equate groups (both visually and through infit);
-   plausibility of the distribution of the D-score by age per study;
-   correspondence of difficulty estimates from published (single study) Dutch data and the new model;
-   infit of the items remaining in the model.

Various routes are possible and may result in different final models. The strategy adopted here is to thicken active equate groups by covering as many studies as possible, in the hope of minimizing the number of active equates needed.

## Impact of number of active equate groups {#sec:impactequate}

```{r model1339, fig.cap = '(ref:model1339)', screenshot.alt="fig/fig_5.2.png"}
knitr::include_url(url = "https://d-score.org/dbook-apps/models1339/#display=by_model_cohort&nrow=2&ncol=2&arr=row&pg=7&labels=cohort,model&sort=cohort;asc,model;asc&filter=&sidebar=&fv=", height = "600px")
```

(ref:model1339) [D-score by age of four models with all 1339 items using 0, 11, 33 and 184 active equate groups. The number of equate groups has a substantial effect on the D-score distribution. Use the arrows to see other cohorts](https://d-score.org/dbook-apps/models1339/#display=by_model_cohort&nrow=2&ncol=2&arr=row&pg=7&labels=cohort,model&sort=cohort;asc,model;asc&filter=&sidebar=&fv=) (<https://d-score.org/dbook-apps/models1339/>).

Figure \@ref(fig:model1339) is a display of the D-score by age for all 16 cohorts under four models. As a rough reference to compare, the grey curves in the back represent the Dutch model as calculated from the [SMOCC study](https://d-score.org/dbook1/sec-smoccstudy.html). In order to speed up the calculations, the figure shows a random subsample of 25% of all points. Manipulate the plot controls to switch cohorts.

All models contain 1339 items, but differ in the number of active equate groups. The most salient features per model are:

-   `1339_0`: No equate groups, so different instruments in different cohorts are fitted independently;
-   `1339_11`: Connects all cohorts through one or more equated items using 11 equate groups in total;
-   `1339_33`: There are 33 equate groups that bridge cohort and instruments;
-   `1339_184`: Maximally connects instruments and cohort by all equate groups.

Comparison of the D-score distribution by age across these models yields various insights:

-   The location of cohorts on the vertical scale depends on the number of active equate groups. For example, for Madagascar (MDG) the points are located around 52 when no equate groups are activated, whereas if all are activated it is about 68.

-   The age trend depends on the number of active equate groups. For example, for Colombia (COL) or Ethiopia (ETH), the model without equate groups has a shallow age trend, whereas it is steep for the `1339_184` model.

-   The vertical spread depends on the number of equate groups. For example, the spread in the Chile-2 (CHL-2) cohort substantially increases with the number of active equates.

-   Model `1339_0` for the Dutch NLD-SMOCC cohort is equivalent to the model fitted to the [SMOCC study](https://d-score.org/dbook1/sec-smoccstudy.html) alone. Introducing equate groups compresses the range of scores, especially at the higher end.

We have now seen that the number of active equate groups has a large effect on the model. The next sections look into the equate groups in more detail.

## Age profiles of similar milestones {#sec:ageprofiles}

```{r p-a-equate-1339, fig.cap = '(ref:p-a-equate-1339)', screenshot.alt="fig/fig_5.3.png"}
knitr::include_url(url = "https://d-score.org/dbook-apps/p-a-equate-1339/#display=Percent_pass_by_age_for_all_equate_groups&nrow=1&ncol=1&arr=row&pg=65&labels=equate&sort=equate_index;asc&filter=&sidebar=&fv=", height = "450px")
```

(ref:p-a-equate-1339) [Percentage of children that pass similar milestones at a given age](https://d-score.org/dbook-apps/p-a-equate-1339/#display=Percent_pass_by_age_for_all_equate_groups&nrow=1&ncol=1&arr=row&pg=65&labels=equate&sort=equate_index;asc&filter=&sidebar=&fv=) (<https://d-score.org/dbook-apps/p-a-equate-1339/>).

Figure \@ref(fig:p-a-equate-1339) displays the percentage of children that pass milestones at various ages. Subject matter experts clustered similar items stemming from different instruments into equate groups. There are 184 equate groups that contain two or more milestones.

Most age profiles show a rising pattern, as expected, though some (e.g. `FM17` or `EXP11`) have one item showing a negative relation with age. Equate `EXP26` combines `two-word sentences` items from seven instruments into one plot. The item difficulties expressed as age-equivalents (c.f. [Section 3.1.2, Chapter I](https://d-score.org/dbook1/sec-agebased.html#sec:ageequivalent)) for these cohorts vary between 20-25 months. By comparison, equate group `EXP18` (`says two words`) shows more heterogeneity across cohorts, and is therefore, less likely to be useful for equating. Equate group `FM31` (`stack two blocks`) is another example of a promising example. By comparison, `FM38` (`stack 6-8 blocks`) shows additional heterogeneity. As a last example, consider `GM42` (`walks alone`), which has a similar age profile across cohorts, whereas `GM44` (`throws ball`) or GM49 (`walk down stairs`) are more heterogeneous.

We could follow different strategies in selecting which equate groups to activate. One strategy would be to include as many equate groups as possible (e.g. all 184 equates) so as to build as many bridges as possible between different instruments. A more selective strategy would be to activate a subset of promising equates and leave others inactive. The following section compares four different approaches.

## Quality of equate groups {#sec:equatequality}

```{r p-d-equate-1339, fig.cap = '(ref:p-d-equate-1339)', screenshot.alt="fig/fig_5.4.png"}
knitr::include_url(url = "https://d-score.org/dbook-apps/p-d-equate-1339/#display=Percent_pass_by_D_score__four_models&nrow=2&ncol=2&arr=row&pg=65&labels=equate,model&sort=equate_index;asc,model;asc&filter=&sidebar=&fv=", height = "450px")
```

(ref:p-d-equate-1339) [Percentage of children that pass similar milestones given their D-score as calculated under four models (1339 items, and 0, 11, 33 and 184 equate groups, respectively](https://d-score.org/dbook-apps/p-d-equate-1339/#display=Percent_pass_by_D_score__four_models&nrow=2&ncol=2&arr=row&pg=65&labels=equate,model&sort=equate_index;asc,model;asc&filter=&sidebar=&fv=) (<https://d-score.org/dbook-apps/p-d-equate-1339/>).

Figure \@ref(fig:p-d-equate-1339) shows how the passing percentage depends on the child's D-score as calculated under four models. All models include the same 1339 milestones, but differ in the number of active equates. The grey curve corresponds to the estimate made under the assumption that milestones are equally difficult. Good milestones for bridging instruments will have a tight bundle of curves. For example, equate `EXP26` has tight bundles especially in models `1339_11` and `1339_33`. By comparison, the curves of the two extreme models vary considerably: the model without any bridges (`1339_0`) or the model with all bridges (`1339_184`) are thus less than ideal. The shallow grey curve of model `1339_184` indicates a poorer overall fit.

Outfit and infit statistics measure the residual deviation of the items to the grey curve. High values (e.g. above 1.4) are undesirable and indicate lack of fit to the model. For example, the fit statistics for `EXP26` in model `1339_184` (1.70 and 1.25) indicate a mediocre fit, whereas `EXP26` in models `1339_33` and `1339_11` fits well. Sometimes the individual item curves are steeper than the grey curve. This indicates that these milestones are more discriminative than the combined item. Model `1339_0` lacks a grey curve and has no fit statistics for equate groups, because in that model, the combined item is not activated.

The probability curves provide a quick visual method for spotting promising and problematic equate groups. Examples of promising equate groups include `COG36`, `FM31`, `GM26` and `GM42`. A little more weak are `FM26` (has more variability), `FM52` (looks promising, but has a problem with the item `grigcd402` from the `GCDG_JAM_STUNTED` cohort), and `GM35` (does not align cohort `GCDG-ZAF`). In such cases, one may wish to move an item out of an equate group, combine equate groups, or inactivate troublesome links.

Until now we only looked at models that include all 1339 items. In practice, we may improve upon the model by selecting the subset of milestones that fit the Rasch model. The next section looks in this modelling step in more detail.

## Milestone selection {#sec:milestoneselection}

```{r itemfit,results = 'hide', fig.keep = 'all', fig.cap='(ref:itemfit)', out.width = "75%"}
m_011 <- load_model("1339_11",  model_dir = "models_2020", project = "Jamaica")

p1 <- 
ggplot(data = m_011$item_fit, aes(x = infit, y = outfit)) +
  geom_hline(yintercept = 1, color = "skyblue", lty = 2) +
  geom_vline(xintercept = 1, color = "skyblue", lty = 2) +
  geom_point(cex = 0.5, col = "grey30") + 
  coord_fixed() +
  scale_x_log10(breaks = c(seq(0.4, 1, 0.1), seq(1.2, 2.2, 0.2)),
                limits = c(0.4, 2.2), minor_breaks = FALSE) +
  scale_y_log10(breaks = c(seq(0.4, 1, 0.1), seq(1.2, 2.2, 0.2)), 
                limits = c(0.4, 2.2), minor_breaks = FALSE) +
  xlab("Item infit") + 
  ylab("Item outfit") + 
  theme_light()

p1

tiff("fig/fig5.5.tiff", unit = "in", width = 5, height = 5, res = 300)
p1
dev.off()
```

(ref:itemfit) Infit and outfit of 1339 items in model `1339_11`. About 8 percent of the points falls outside the plot.

Item infit and outfit are convenient statistics for selecting the milestones that fit the model. Figure \@ref(fig:itemfit) displays the infit and outfit statistics of model `1339_11`. The correlation between infit and outfit is high ($r = 0.84$). The expected value of the infit and outfit statistics for a perfect fit is 1.0. The centre of infit and outfit in Figure \@ref(fig:itemfit) is approximately 1.0, so on average one could say the items fit the model. Note however that fit values above and below the values of 1.0 are qualitatively different. Item with fit statistics exceeding 1.0 fit the model less well than expected (**underfit**), whereas items with fit statistics lower than 1.0 fit the model better than expected (**overfit**). See [Chapter 1, Section 6.1](https://d-score.org/dbook1/sec-itemfit.html) for more details.

Some practitioners remove both underfitting and overfitting items. However, we like to preserve overfitting items and be more strict in removing items that underfit. The idea is that preservation of the best fitting items may increase scale length, and hence reliability and measurement precision. Figure \@ref(fig:itemfit) draws two cut-off lines at 1.0. Taking items with infit \< 1.0 and outfit \< 1.0 will select **631 out of 1339** items for further modelling.

A practical problem of item removal is that it also affects equate group composition. By default, a removed item will also be removed from the equate group, so item removal may reduce the size of an equate group below two items. For passive equates this is no problem, since passive equates do no affect the estimates. However, removal of an underfitting item from an active equate group will break the bridge between the instrument it pertains to and the rest of the item set. Potentially this can result in substantial effects on the D-score distribution of the cohort, as demonstrated in Figure \@ref(fig:model1339). As a solution, we force any items that are members of active equate groups to remain in the analysis. If that leads to substantially worse equate fit in the next model, we must search for alternative equate groups that bridge the same instruments and that are less sensitive to misfit.

## Other modelling actions {#sec:othermodelling}

### Instrument fit

```{r itembox, results = 'hide', fig.keep = 'all',fig.cap='(ref:itembox)'}
ib <- data.frame(m_011$item_fit, decompose_itemnames(m_011$item_fit$item))
ib <- ib[ib$instrument %in% c("aqi", "bar", "bat", "by1", "by2", "by3", "ddi", "den", "gri", "sbi", "tep", "vin"), ]

p1 <- 
ggplot(data = ib, aes(x = instrument, y = outfit)) +
  scale_y_log10(breaks = c(seq(0.4, 1, 0.1), 1.1, 1.3, 1.5, seq(2:10)), 
                limits = c(0.4, 9), minor_breaks = FALSE) +
  geom_boxplot(width = 0.8) +
  xlab("Instrument") +
  ylab("Item outfit") +
  theme_light()
p1

tiff("fig/fig5.6.tiff", unit = "in", width = 8, height = 5, res = 300)
p1
dev.off()
```

(ref:itembox) Box plot of the distribution of item outfit per instrument in model `1339_11`.

Some instruments fit better than others. Figure \@ref(fig:itembox) shows the box plots of outfit per instrument. Instruments `bar`, `by1`, `ddi` and `vin` generally fit well, whereas discrepancies between model and data are larger for `bat`, `by2` and `sbi`. Through additional modelling, we found that it was extremely difficult to get enough high-quality bridge items that could link `bat` (Battelle Development Inventory) to the other instruments. We also found that models without the Battelle were able to better discriminate children in the upper range of the D-score scale. We therefore opted to remove `bat` from the model, even though this meant that one cohort (`GCDG-BRA-2`) had to be dropped from the analysis.

It is not clear why `bat` does not fit. Perhaps the scoring system of the Battelle in three categories invokes scoring behaviour that is different from the PASS/FAIL scoring used by most other instruments, even though this appears to be less of a troublesome aspect in `aqi`, which also uses three response categories.

### Splitting, combining and selecting equate groups

Most of the modelling effort went into finding a set of high-quality equate groups that link the instruments. For example, we tried to bridge the South-African study placing `vinxxc016` (uses a short sentence) into `EXP26` (two-word sentences) and `EXP36` (sentences of 3 or more words), but neither option led to a reasonable model. On the surface, milestone `by3gmd060` (balances on right foot, 2 seconds) appears to fit within `GM60` (balances on foot), but the analysis showed large discrepancies with the other items in the groups, so it had to be taken out.

Subject-matter experts identified 38 items that were thought to be cross-culturally incompatible. Table \@ref(tab:crosscultural) provides an overview. Many of such milestones involve a specific language concept (such as a pronoun), refer to stairs (less common in rural settings), help in house or clothing behaviour. These items have different meanings in different contexts, so they were not used to bridge instruments.

```{r crosscultural, echo=FALSE}
# taken from 571_17.R
out <- c("aps23", "aps41", "b1m50", "b1p53", "b1p54",
         "b2p69",
         "b3c43", "b3c52", "b3g47", "b3g49", "b3g57",
         "b3g58", "b3r30", "b3e30",
         "bm16", "bm20",
         "dg20", "dp12", "dp13",
         "gg2_19", "gg2_22",
         "mil2", "mil3", "mil4", "mil5", "mil6",
         "n53", "n54",
         "sa2v2", "sa2v3", "sa2v9", "sa2v12", "sa2v14",
         "sa2v22", "sa2v28",
         "savibe4playswith", "savine4helpshouse",
         "v25")
items <- rename_gcdg_gsed(out)
tab <- data.frame(Item = items, Label = get_labels(items), row.names = NULL)

if (opts_knit$get("rmarkdown.pandoc.to") %in% c("html", "latex")) {
  
knitr::kable(tab, 
             caption = "Milestones not used for equating because of limited cross-cultural validity",
             booktabs = TRUE) %>%
  kableExtra::column_spec(1, monospace = TRUE) %>% 
  kableExtra::scroll_box(height = "400px", width = "100%")
}


if (!opts_knit$get("rmarkdown.pandoc.to") %in% c("html", "latex")) {
  
 ft <- flextable(tab)
 ft <- set_caption(ft, "Milestones not used for equating because of limited cross-cultural validity")
 ft <- set_table_properties(ft, layout = "autofit", width = .9)
 ft <- font(ft, j = 1, fontname = "Courier")
 knit_print(ft)
}

```

```{r plotequates, results = 'hide', fig.keep = 'all', fig.height = 11, warning = FALSE, fig.cap = '(ref:plotequates)', message = FALSE}

pdif <- dmetric::plot_p_d_equate(data = dmetric::gcdg_lean , model = dmetric::model_lean, passive = TRUE)

```

(ref:plotequates) Active and non-active equate groups.

## Item information {#sec:iteminformation}

Item information is a psychometric measure that quantifies the sensitivity of the item to changes in the person's ability. An item is most sensitive around the D-score value where the PASS probability equals the FAIL probability, which corresponds to the item difficulty ($\delta_i$). One unit change around $\delta_i$ has a large effect on the probability of endorsing, while one unit change far away from $\delta_i$ has negligible impact. Suppose person A had passing probability $0.7$ for some item. The information delivered by that item for person A is the product $0.7 \times (1.0 - 0.7) = 0.21$. Suppose person B has a D-score that coincides with the difficulty level of the item. In that case, the information for B equals $0.5 \times (1 - 0.5) = 0.25$, the maximum. Likewise, for a person C with high ability, the information could be $0.98 * 0.02 = 0.02$, so that item carries almost no information for person C.

The information is inversely related to the error of measurement. More information amounts to less measurement error. For each response in the data, we can compute the amount of information it contributed to the model D-score. By summing the information over persons, we obtain a measure of certainty about the difficulty estimate of the item. This sum of information incorporates both the number of administrations and the quality of the match between person abilities and item difficulty.

```{r iteminfo,results = 'hide', fig.keep = 'all', echo = FALSE, warning = FALSE, message = FALSE,fig.cap='(ref:iteminfo)', fig.keep='all', fig.height=3}

itembank <- dscore::builtin_itembank[dscore::builtin_itembank$key == "gcdg",]
#calculate item information given the dscores from gsed 807 model
gcdg_iteminfo <- dmetric::gcdg_lean$itm %>% dplyr::filter(item %in% itembank$item) %>%
  left_join(dmetric::model_lean$dscore, by = c("subjid", "agedays")) %>%
  left_join(itembank, by = "item") %>% select(subjid, agedays, item, value, cohort, d, tau) %>% mutate(info = dinstrument::info(beta = .data$d, delta = .data$tau)) %>% group_by(item) %>% summarize(n = n(), info = sum(info, na.rm = TRUE), tau = mean(tau)) %>%
  mutate(grade = cut(info, breaks = c(0,5,25,100,10000),
                     labels = c("D", "C", "B", "A")),
         grade = factor(grade, labels = c("D", "C", "B", "A")))

eqdf <- vector()
for(i in names(dmetric::model_lean$fit$equate)){
  eqdf1 <- data.frame(equate = i, item = dmetric::model_lean$fit$equate[[i]])
  eqdf <- rbind(eqdf, eqdf1)
}

gcdg_equateinfo <- 
  gcdg_iteminfo %>% filter(item %in% eqdf$item) %>% left_join(eqdf, by = "item") %>%
  group_by(equate) %>%summarize(n = sum(n), info = sum(info, na.rm = TRUE), tau = mean(tau)) %>%
  mutate(grade = cut(info, breaks = c(0,5,25,100,10000),
                     labels = c("D", "C", "B", "A"))) %>%
  arrange(.data$tau) %>%
  select(equate, tau, n, info, grade)

gcdg_iteminfo2 <-gcdg_iteminfo %>% filter(!item %in% eqdf$item)

p1 <- 
ggplot()+
  geom_point(data = gcdg_iteminfo2, aes(x = tau, y = grade, group = grade), color = "grey") +
  geom_point(data = gcdg_equateinfo, aes(x = tau, y = grade, group = grade), shape = 21, size = 2, color = mice::mdc(2), stroke = 2) +
  xlab("Item difficulty") + ylab("Information grade") +
  theme(legend.position = "none")
p1

tiff("fig/fig5.7.tiff", unit = "in", width = 10, height = 3, res = 300)
p1
dev.off()

```

(ref:iteminfo) Item information grade by item difficulty for the final model

Figure \@ref(fig:iteminfo) displays the summed information for each item, divided into four grades: A(best) to D (worst). The information grade measures the stability of the difficulty estimate. Most items receive grades higher than C. In total, 30 milestones have grade D. Adding these items to future studies may yield important additional information.

The red circles indicate active equate groups. Most have grade A, so we have a lot of information about the items that form the active equate groups. Table \@ref(tab:equateinfo) displays more detailed information for the active equate groups. The sample sizes are reasonably large. Many information statistics are well is above 100; the criterion for Grade A. The interpretation of this criterion is as follows. Suppose that we obtain a sample of 400 persons who are all perfectly calibrated to the item of interest. In that case, the information for that item will be equal to 100.

```{r equateinfo, echo = FALSE}
if (opts_knit$get("rmarkdown.pandoc.to") %in% c("html", "latex")) {

knitr::kable(gcdg_equateinfo, 
             col.names = c("Equate group", "Difficulty", "Sample Size", "Information", "Grade"),
             caption = "Equate group information in the final model.",
             booktabs = TRUE) %>%
  kableExtra::kable_styling() %>%
  kableExtra::scroll_box(width = "100%", height = "500px")
}

if (!opts_knit$get("rmarkdown.pandoc.to") %in% c("html", "latex")) {
  
 ft <- flextable(gcdg_equateinfo)
 ft <- set_header_labels(ft, equate = "Equate group", tau = "Difficulty", n= "Sample Size", info = "Information", grade = "Grade")
 ft <- set_caption(ft, "Equate group information in the final model.")
 ft <- set_table_properties(ft, layout = "autofit", width = .9)
 knit_print(ft)
}

```

## Final model {#sec:finalmodel}

Unfortunately, there is no single index of model fit that we can optimise. Modelling is more like a balancing act among multiple competing objectives, such as

-   preserving as many items as possible that fit the model;
-   finding high-quality active equate groups that span many cohorts and instruments;
-   picking active equate groups for which we have enough information;
-   providing reasonable age-conditional distributions of the D-score;
-   representing various developmental domains in a fair way;
-   preserving well-fitting historical models as new data become available;
-   maintaining a reasonable calculation time.

This section showed various modelling techniques and ways to assess the validity of the model. In real life, we fitted a total number of 140 models on the data and made many choices that weigh the above objectives. The final model for the GCDG data consists of 565 items (originating from 14 instruments) that fit the Rasch model and that connect through 18 equate groups. Due to the sparseness of data at the very young ages, the quality of the model is best for ages between 4-36 months.

Model `565_18` formed the basis of the publication by @Weber2019. Additional detail on model `565_18` is available through the [`dmodel` shiny app](https://tnochildhealthstatistics.shinyapps.io/dmodel/).
